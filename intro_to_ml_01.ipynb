{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eight-music",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning<br><br>Day 01:<br>Key Concepts and Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupational-arcade",
   "metadata": {},
   "source": [
    "<center>Dr. William Mattingly<br>\n",
    "TAP Institute with JSTOR</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welsh-nelson",
   "metadata": {},
   "source": [
    "## Preface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "featured-neighborhood",
   "metadata": {},
   "source": [
    "This notebook was put together for the 2021 TAP Institute's summer courses. This notebook is meant to function as a quick introduction to machine learning. It will not sereve as a complete overview, rather a quick way to get a genereal understanding about what machine learning is and how it works. In notebooks 2 and 3, we will solve two different problems with machine learning--topic modeling and text classification. Rather than being about these particular methods of text analysis, we use these problems as a way to frame the machine learning training (unsupervised and supervised learning). This notebook will be more text-centric, while the next two will be focused on two Python libraries for engaging in ML, Gensim (Topic Modeling) and spaCy (text classification).\n",
    "\n",
    "Notebook 2 (Topic Modeling) will leverage an existing cultivated dataset 20-Newsgroup. While this dataset is not relevant to the humanities particularly, it provides a good toy example for how to load, clean, and work with an existing dataset. This is an essential skill, despite, as we shall discuss in Notebook 3 (Text Classification), the fact that humanists do not currently have access to a large number of well-cultivated datasets for research purposes.\n",
    "\n",
    "Notebook 3 (Text Classification) will introduce readers to a common problem in the digital humanities: cultivating a good dataset to train a machine learning model. In this notebook, we will create a binary classification model that can determine if a Holocaust document deals with the concept of \"hunger\". We will cultivate a dataset using a few rules that will require manual validation. In this notebook, I will also introduce the reader to some concepts and terms not covered in this notebook, particularly types of classifications, prediction, and generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "following-making",
   "metadata": {},
   "source": [
    "## Covered in this Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "private-palmer",
   "metadata": {},
   "source": [
    "1) What is Machine Learning?<br>\n",
    "2) How does Machine Learning work?<br>\n",
    "3) What are models and how do we train them?<br>\n",
    "4) How do we represent text in computer systems?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "several-apparel",
   "metadata": {},
   "source": [
    "## Before we Begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspended-catholic",
   "metadata": {},
   "source": [
    "Before we begin our journey into machine learning, let's start with a fun thinking exercise. I have found it is best to do this as a true story. Take a look at the image below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atmospheric-smith",
   "metadata": {},
   "source": [
    "<img src=\"https://i.pinimg.com/originals/68/c8/a0/68c8a0eb4c4ce56e4d54e9df98dfa802.jpg\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"float: left; margin-right: 10px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "synthetic-closer",
   "metadata": {},
   "source": [
    "If I were to ask anyone what this is an image of, we would likely all have the same answer. \"Penguin!\" Of course it is. But that's not quite happened when my neighbor's three-year-old child went to my parent's front yard where this very decoration sits every Christmas. To keep this child anonymous, we will simply call him Timmy. Young Timmy did not say \"penguin\" as we all did. Instead, he said \"duck!\"\n",
    "\n",
    "Was Timmy wrong? From our vantage point, yes. He was horribly wrong. Is this his fault? Of course not. He was three. Now, if a grown adult, professor of English came over to the house and made the same comment, we might look at her funny and wonder what she was talking about. We may look behind the penguin to see if there was a duck from the nearby pond lost. Once we realized that she was clearly making a statement about the penguin, we would likely fault her for making an incorrect comment about what that wire-mesh animal truly was.\n",
    "\n",
    "In this scenario, we have a machine learning problem. Young Timmy is a child. He has limited **experiencese** in this world. He has a pet duck, a few dogs, and a few cats. He knows what dogs and cats are and he knows what ducks are because of this. In machine learning terms, Timmy's experiences constitute seeing images in his environment and learning what **label** to assign to those things. His parents, like all parents, likely pointed at the dog before he could remember and said, \"dog... dog... dog...\" and likewise with cats and ducks. Let's presume Timmy only knows those three animals. Nothing else.\n",
    "\n",
    "What happened in this scenario? Well, Timmy who was able to classify only three animals out of the millions that exist, met a new out-of-scope animal. Something he had never encountered. He was being asked, in machine learning terms, to generalize on unseen data. Unfortunately for Timmy, that unseen data was unfair. It was something that did not fit into any of the three labels he knew. What did Timmy do in this scenario? He did what any good machine learning model would do. He gave the best answer he could. He said \"duck\".\n",
    "\n",
    "Honestly, Timmy did a great job. He showed the abilitiy to understand the saliant features of a duck. **Features** in machine learning terms are the aspects of an item that are important. The pieces of it that make it what it is. In Timmy's case, he likely saw the wings, the bill, and the two feet and came to a quick conclusion. \"Duck!\" These three features clearly make a duck (or a penguin) not a dog or cat. And for these reasons, he gave the label of \"duck!\".\n",
    "\n",
    "What did Timmy's parents do in this scenario? Like all great parents they used the experience to engage in what we call **reinforcement learning** (in machine learning terms). They bent down and pointed at the penguin and said \"no... penguin... penguin... penguin...\". Young Timmy grimmaced, looked confused, and moved on with his evening. The nearby cookies had already arrested his attention.\n",
    "\n",
    "Will Timmy call this specific thing a penguin next time he sees it? Possibly. Will he be able to identify a true penguin in the real world? Maybe not. Real penguins look different and Timmy has only had one experience with penguins. He will need many more before he can confidently identify them consistently.\n",
    "\n",
    "At the end of the day, this is how machine learning works. Its the replication of this process in a computer system through statistics and mathematics.\n",
    "\n",
    "If you have not understood all of these terms or concepts at this time, that's okay. That's what these notebooks are for! After working through this notebook, return to this scenario again and see if you understand it a bit more. Do the same at the end of notebook 3 when we engage in **supervised learning**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earlier-citizenship",
   "metadata": {},
   "source": [
    "## Part One - What is Artificial Intelligenece?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "asian-trader",
   "metadata": {},
   "source": [
    "Artificial intelligence, or AI, is a term that has received much hype in recent years. Beyond its science fiction usages, the term AI has increasingly entered the average lexicon since 2010. The reason for this is what Fran√ßois Challete has dubbed, the unfreezing of the Second AI Winter. The new surge in AI since 2010 has largely been in the realm of machine learning, or ML, specifically the subfield of ML, deep learning. For an understanding of how these fields and subfields relate to one another, see the image below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amateur-tunnel",
   "metadata": {},
   "source": [
    "<img src=\"https://www.edureka.co/blog/wp-content/uploads/2018/03/AI-vs-ML-vs-Deep-Learning.png\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"float: left; margin-right: 10px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "living-classic",
   "metadata": {},
   "source": [
    "To understand what ML is, however, we need to understand what it is not. Machine learning and AI, at the current state of research are nothing like science fiction movies and TV shows. We are not yet at the point of achieving something like Rosie, the maid from the Jetsons, or the more fearsome elements of Skynet from the Terminator franchise. These types of AI are likely the future of the field, but this is far from state of current research."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extraordinary-congo",
   "metadata": {},
   "source": [
    "<img src=\"https://1.bp.blogspot.com/-x1BOIRYorms/XrX_k7RrYZI/AAAAAAABA38/rOe7sf1YobksLKr2LN-rbHKZskfT6SuGgCLcBGAsYHQ/s1600/jetsons03.jpg\"\n",
    "     style=\"float: left; margin-right: 10px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integrated-fabric",
   "metadata": {},
   "source": [
    "Rosie would represent what we would call Artificial General intelligence. She can perform myriad of human tasks at human-like proficiency, from cleaning the house, to making decisions, and even throwing a baseball with Elroy, the son in the Jetsons. With some tasks, she can exceed human-like intelligence, specifically in the computation of numbers. For the most part, however, Rosie's abilities to not exceed her human counterparts. For this reason, we call this Artificial General Intelligence (AGI)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weird-general",
   "metadata": {},
   "source": [
    "<img src=\"https://www.denofgeek.com/wp-content/uploads/2009/05/37756-1.jpg\"\n",
    "     style=\"float: left; margin-right: 10px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "median-fortune",
   "metadata": {},
   "source": [
    "The scarier aspects of Skynet in Terminator reprent the next progreession in AI from AGI to Aritifical Super Intelligence. This is an AI system that outperforms humans at human tasks. Were it to throw a baseball, it would be 200 mph and always make accurate predictions on human behavior to have a strike (sometimes forcing the batter to swing and miss) nearly every pitch.\n",
    "\n",
    "We are not yet either of these levels. Right now, AI sits in the realm of Artificial Narrow Intelligence (ANI), where it has remained since the 1950s. The key difference between now and the 1950s is the way in which, the speed with which, and the accuracy of these AI systems. In the 1950s and up until 2010, the realm of AI was dominated by rules-based approaches to problems.\n",
    "\n",
    "By its broadest definition, AI is the performing of a human task by a computer system. Within this definition, a calculator would be AI. With a rules-based system, a practitioner must write a set of rules that guide a computer system for making decisions when faced with challenges. For simple tasks, such as solving a maze, this is quite simple. One rule solves the problem. Always turn right. If the maze is solvable, this single rule will win every time.\n",
    "\n",
    "For more complex tasks, such as playing games, the rules that allow the computer system to win must become more complex. For a rules-based AI system to win at checkers or chess, it must be aware of potential future moves and choose the one that has the best chance of winning. For it to make such a predicion, the researcher must write out how to look at future moves and how to determine the one that has the best chance of winning. There are different ways to do this which are beyond the scope of this notebook. Instead, this notebook will focus on the more nascent method for solving such problems, machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suitable-tract",
   "metadata": {},
   "source": [
    "## Part Two - What is Machine Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immune-assembly",
   "metadata": {},
   "source": [
    "Machine Learning is the process by which practitioners teach a computer system to perform a task, not with rules, but with statistics and linear algebra so that the system can learn from repeated (and randomized) experiences. If this does not make sense right now, it will by the end of this notebook. This notebook will necessary contain some math, but it will be kept to an absolute minimum. I will only present the math and mathematical concepts that are absolutely necessary.\n",
    "\n",
    "Machine Learning is a term that is increasingly used in the media. The is largely because of the rise of its use since 2010 when ML solutions started solving some of the world's more complex problems seemingly over night. By 2010, computers had evolved to the point where researchers could use high-end GPUs to solve complex computer problems, but its theory and use dates to much earlier.\n",
    "\n",
    "Donald Michie, pictured below, was a contemporary of Alan Turing. In the 1960s, he theorized that he could prove that computer systems could learn through past experiences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "curious-nebraska",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/4/40/Donald_Michie_1986.jpg\"\n",
    "     style=\"float: left; margin-right: 10px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "covered-hospital",
   "metadata": {},
   "source": [
    "He did this by inventing MENACE (Matchbox Executable Noughts and Crosses Engine), shown below. This was a \"computer\" in a broad sense, but not in a typical one. Rather than it being electrical in nature with 0s and 1s, it was a collection of 300 matchboxes. Yes, matchboxes. Those things you use to start a fire. Each matchbox had a different picture on its surface that represented a potential state of the game.\n",
    "\n",
    "Inside each matchbox were beads, each of which represented a different choice. At the end of the game, if the choice resulted in a win or a draw, that choice remained in the box. If, however, it resulted in a loss, then it was removed. The player then played against the computer. The choice of computer is entirely random. If the randomness aligns well early on in its \"learning\", then only the good choices would remain after one-hundred rounds. By then, you have a computer system made of nothing but matchboxes and beads that can either beat or draw against any human.\n",
    "\n",
    "This fun example demonstrates that non-living objects can learn from past experiences. This is the fundamental principal behind machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "synthetic-simulation",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/da/Mscroggs-MENACE-cropped.jpg/525px-Mscroggs-MENACE-cropped.jpg\"\n",
    "     style=\"float: left; margin-right: 10px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "center-cathedral",
   "metadata": {},
   "source": [
    "Since we know that computer systems can learn from experiences, the question to structure those experiences. This is known as learning and when it comes to ML there are several different approaches to learning. You will select specific methods based on the problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "speaking-matthew",
   "metadata": {},
   "source": [
    "## Part Three - Types of Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improved-profession",
   "metadata": {},
   "source": [
    "There are several different types of machine learning: supervised learning, unsupervised learning, and semi-supervised learning. There are others (such as reinforcement learning), but these are the three essential forms.\n",
    "\n",
    "Supervised learning is when we train a system with known data. In the case of text classification, we train a system with a series of texts with the labels appropriately assigned. Unsupervised learning occurs when you don‚Äôt know the categories of your data. You give the system a series of data and allow it to learn and identify patterns on its own. This is most popularly used for topic modeling and k-means. Finally, there is semi-supervised learning which falls between these two.\n",
    "\n",
    "Throughout the next few notebooks, we will work with unsuperevised learning (topic modeling) and supervised learning (text classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "healthy-election",
   "metadata": {},
   "source": [
    "## Part Four - What is Deep Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "economic-contributor",
   "metadata": {},
   "source": [
    "Deep learning gis a subfield of Machine Learning. With deep learning, practitioners create architectures of different layers stacked on top of each othere to achieve improved results. This is done largely through neural networks. We will not be designing custom neural networks in the next few days, but after this course, you will have a strong enough basis in ML to start engaging in deep learning with libraries, such as TensorFlow/Keras or PyTorch/FastAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changed-model",
   "metadata": {},
   "source": [
    "## Part Five - Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "racial-anxiety",
   "metadata": {},
   "source": [
    "As noted above, supervised learning is the process by which a system learns from a set of inputs that have known labels. \n",
    "\n",
    "If one is engaging in text classification, training data may be a text followed by a label that corresponds to that text. Were we wanting to train a binary classification model, or a model that has to decide between two potential classes or topics, such as positive or negative, training data, may look like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premier-coral",
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    [\n",
    "    \"This is a positive review.\",\n",
    "    1\n",
    "    ],\n",
    "    \"This is a negative review.\",\n",
    "    0\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expired-fossil",
   "metadata": {},
   "source": [
    "In this simple example, we have a list that contains lists. Each sublist contains two elements, the text and the label. Here, 1 represntes positive and 0 represents negative. In reality, you will have hundreds or thousandsd of annotated training data like these above. The goal is to have a great deal of real-world variance in your data.\n",
    "\n",
    "In order to train properly, the input data is divided into three categories: training data, validation data, and testing data. There is no set percentage to asign to each of these categories. A good rule of thumb, however, is save 20% of all annotated data for testing and then divide the remaining 80% 80/20 (testing/validation) ratio.\n",
    "\n",
    "The first two, training data and validation data, are given to the system that is trying to learn. It uses the training data to hone a statistical model via predetermined algorithms. It does this by making guesses about what the proper labels are. It then checks its accuracy against the labels provided and makes adjustments accordingly.\n",
    "\n",
    "Once it is finished viewing and guessing across all the training data, the first epoch, or iteration over the data, is finished. At this stage, the model then tests its accuracy against the validation data. These are left out of the training process and give the system a sense of its overall performance.\n",
    "\n",
    "Because the validation data is left out of the training process, it able to be used for mid-training testing (or validation) of its accuracy. The training data is then randomized and given back to the system for x number of epochs. Again, there is no standard for the number of epochs, but a good rule of thumb is to start at 10 and study the results.\n",
    "\n",
    "Once the model repeats this process for the set number of epochs, it is finished training. The model‚Äôs accuracy can then be tested against the testing dataset to see how well it performs. The reason you want to keep the testing data separate from the validation data is because, despite being not include in the training, some of the validation data seeps into the training process. Because the testing data is well-annotated, the researcher can get an accurate sense of how well that model performs.\n",
    "\n",
    "With that first model saved, it is common practice to adjust the parameters of the model multiple times to try to create a more accurate model. All models will be tested against the same testing data.\n",
    "\n",
    "At this stage, depending on the results, more training data may need to be obtained, another test may be called for, or the researcher can begin deploying the model on unseen data and examine the results. Unseen data will be data that does not have annotations.\n",
    "\n",
    "A good way to describe it is via the image below from https://bigdata-madesimple.com/. In the image, we see the input of raw data to an algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "devoted-serve",
   "metadata": {},
   "source": [
    "<img src=\"https://bigdata-madesimple.com/wp-content/uploads/2018/02/Machine-Learning-Explained1.png\"\n",
    "     style=\"float: left; margin-right: 10px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sweet-harrison",
   "metadata": {},
   "source": [
    "Before we give the computer this training data, we must first convert it into numerical data. In to understand how and why, we must understand how computers operate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "familiar-print",
   "metadata": {},
   "source": [
    "## Part Six - Text and Computers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tribal-palestine",
   "metadata": {},
   "source": [
    "It may be surprising to hear this, but computers cannot read text. They cannot in any way process textual data. The reason for this comes down to how computers function at the hardware level. They are a series of electrical impulses--on and off. This on and off is represented numerically as 0 and 1. This is known as binary language and it is the fundamental way in which computers function. More recently, this has changed with quantum computing, but these computers are exceptionally expensive and rest in only the highest-level research labs at the moment.\n",
    "\n",
    "Because all computers (with the exception of quantum computers) are binary and, thereby, numerical in nature, there is no way for a computer to understand the character \"t\". In order for a \"t\" to be registered in a computer system, it must be rendered as a number. To achieve this, early scholars found an elegant solution--reprent that letter as a series of 0s and 1s and thus binary rendering of things beyond 0 and 1 was born. The letter \"t\", for example, is \"01110100\". This same thing needs to be done for a computer to understand other numbers too. For example, \"2\" in binary is \"10\".\n",
    "\n",
    "Over time, scholars developed different ways to encode text in computer systems. All of these encoding initiatives had the same goal: get the non-computer character, i.e. a letter or punctuation, represented in a computer language, i.e. 0s and 1s. Once you have a way to represent a character numerically, the question then becomes how do you represent a word or sentence numericially? On the surface, the answer to this question seems simple. Make that word or sentence a series of characters (which the computer can represent numerically). The result would be larger constructs of human writing, i.e. a word, a sentence, a paragraph, a page, a book, etc.\n",
    "\n",
    "For most tasks, this simple solution is the best solution. Were I needing to simply render text on a screen, representing a word as a series of characters is perfectly fine. The computer will see them as a series of numbers, and I, as the reader, will see them as a series of characters. In machine learning, however, we are less interested in presenting text, and more interested in representing the meaning of text.\n",
    "\n",
    "When we see the word \"ball\" for example, we do not see a \"b\", an \"a\", an \"l\", and another \"l\". We see a concept. We have a mental image. For me, it's a beachball. For you, it may be a baseball, a basketball, a lacrosse ball, or some other spherical object that constitutes a \"ball\". We arive at this understanding from years of experience. We had a parent point at a ball before we can remember and repeat the word over and over... \"ball... ball... this is a ball... ball...\" As we grew older, we held a ball in our hand and said \"ball\". As we grew older and maybe played sports, we yelled at our teammates, \"THROW ME THE BALL! I'M OPEN!\" We watched movies, maybe the eternal classic Dodgeball. We watched Marsha Brady get hit in the face during the iconic Brady Bunch episode. My point is, our respective lives have been filled with experiences that involved the word \"ball\". When we see the word appear on a page or a computer screen, the experience is likely the same. We translate that word into meaning.\n",
    "\n",
    "Now, for the difficult question. How do we get a computer system to do that? When engaging in Machine Learning for the purpose of analyzing texts, this is a fundamental problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "senior-benefit",
   "metadata": {},
   "source": [
    "## Part Seven - Bag-of-Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mature-essex",
   "metadata": {},
   "source": [
    "When it comes to ML, we often need to represent constructs larger than a character numerically. By this I mean we must represent a word, a sentence, or even an entire text as a number. Scholars have devoted entire careeres to solving this problem. Early scholars (who did not have powerful computers) found a computationally inexpensive way to solve this problem. Represent each word as a unique number. This is known as a Bag-of-Words, or BOW.\n",
    "\n",
    "So, how does a BOW work? We have two options. We can either move through a dictionary of a language and each progressive word receives the next highest intiger. In this scenario, the word \"ape\" might be number \"113\" and the word \"apple\" be \"114\". After iterating through all words in the dictionary, each word will be represented by number. This allows one to take the followingg sentence \"The apple is large\" and render it numerically as 250 114 200 220. Likewise the slightly different sentence, \"The ape is large\" can be rendered as 250 113 200 220.\n",
    "\n",
    "This dictionary approach has two problems. First, you may have words represented that your particular texts never use. In other words, you have wasted computer resources. This is not so much a problem today as it was back in the 1960s. The second problem is that your text may use a word that is not captured in the dictinary. Perhaps your text is Harry Potter and your dictionary is from 1980. The word \"muggle\" is likely not in that dictionary.\n",
    "\n",
    "For these reasons, a scholar may represent words numerically by assigning a new number to each new word encountered. So, were we to be reading The Hobbit, \"In a hole in the ground...\" would be represented as 1 2 3 1 4 5. This solution redudces the quantity of words to the max number of words contained within your text.\n",
    "\n",
    "Both BOW methods have one fundamental flaw. And maybe you have already caught it. They do not offer any meaning. The proximity of apple and ape in the first example in the dictionary do not mean that they are in anyway related to each other in meaning. Likewise, in our second example, there is nothing about hole and ground that have any meaning, despite their numbers being close to each other.\n",
    "\n",
    "And here lies the problem with BOW. It is a computationally inexpensive way to represent words numerically. This means that BOW still has a use today, specifically if a researcher wishes to get a quick sense of the textual data at hand by applying certain algorithms, such as TF-IDF, or Term-Frequency Inverse-Document Frequency. The main drawback to BOW, however, is that it does not allow the computer system to draw any meaning between words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boolean-radical",
   "metadata": {},
   "source": [
    "## Part Eight - Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focused-things",
   "metadata": {},
   "source": [
    "Word vectors, or word embeddings, take these one dimensional bag of words and gives them multidimensional meaning by representing them in higher dimensional space, noted above. This is achieved through machine learning and can be easily achieved via Python libraries, such as Gensim, which we will explore more closely in the next notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternative-enzyme",
   "metadata": {},
   "source": [
    "The goal of word vectors is to achieve numerical understanding of language so that a computer can perform more complex tasks on that corpus. Let‚Äôs consider the example above. How do we get a computer to understand 2 and 6 are synonyms or mean something similar? One option you might be thinking is to simply give the computer a synonym dictionary. It can look up synonyms and then know what words mean. This approach, on the surface, makes perfect sense, but let‚Äôs explore that option and see why it cannot possibly work.\n",
    "\n",
    "For the example below, we will be using the Python library PyDictionary which allows us to look up definitions and synonyms of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaging-overall",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyDictionary import PyDictionary\n",
    "\n",
    "dictionary=PyDictionary()\n",
    "text = \"Tom loves to eat chocolate\"\n",
    "\n",
    "words = text.split()\n",
    "for word in words:\n",
    "        syns = dictionary.synonym(word)\n",
    "        print (f\"{word}: {syns[0:5]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "private-footwear",
   "metadata": {},
   "source": [
    "Even with the simple sentence, the results are comically bad. Why? The reason is because synonym substitution, a common method of data augmentation, does not take into account syntactical differences of synonyms. I do not believe anyone would think ‚ÄúFelis domesticus‚Äù, the Latin name of the common house cat, would be an adequite substitution for the name Tom. Nor is ‚Äúgarbage down‚Äù a really proper synonym for eat.\n",
    "\n",
    "Perhaps, then we could use synonyms to find words that have cross-terms, or terms that appear in both synonym sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "black-series",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyDictionary import PyDictionary\n",
    "\n",
    "dictionary=PyDictionary()\n",
    "\n",
    "words  = [\"like\", \"love\"]\n",
    "for word in words:\n",
    "    syns = dictionary.synonym(word)\n",
    "    print (f\"{word}: {syns[0:5]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "athletic-letter",
   "metadata": {},
   "source": [
    "This, as we can see, has some potential to work, but again it is not entirely reliable and to work with such a list would be computationally expensive. For both of these reasons, word vectors are prefered. The reason? Because they are formed by the computer on corpora for a specific task. Further, they are numerical in nature (not a dictionary of words), meaning the computer can process them more quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken-shopper",
   "metadata": {},
   "source": [
    "Word vectors have a preset number of dimensions. These dimensions are honed via machine learned. Models take into account word frequency alongside words across a corpus and the appearance of other words in similar contexts. This allows for the the computer to determin the syntactical similarity of words numerically. It then needs to represent these relationships numerically. It does this through the vector, or a matrix of matrices. To represent these more concisely, models flatten a matrix to a float (decimal number). The number of dimensions represent the number of floats in the matrix.\n",
    "\n",
    "Below is a pretrained model‚Äôs output of word vectors for Holocaust documents. This is how the word ‚Äúknow‚Äù looks in vectors:\n",
    "\n",
    "know -0.19911548 -0.27387282 0.04241912 -0.58703226 0.16149549 -0.08585547 -0.10403373 -0.112367705 -0.28902963 -0.42949626 0.051096343 -0.04708015 -0.051914077 -0.010533272 -0.23334776 0.031974062 -0.015784053 -0.21945408 0.07359381 0.04936823 -0.15373217 -0.18460844 -0.055799782 -0.057939123 0.14816307 -0.46049833 0.16128318 0.190906 -0.29180774 -0.08877125 0.23563664 -0.036557104 -0.23812544 0.21938106 -0.2781296 0.5112853 0.049084224 0.14876273 0.20611146 -0.04535578 -0.35051352 -0.26381743 0.20824358 0.29732847 -0.013382204 -0.19970295 -0.34890386 -0.16214448 -0.23497184 0.1656344 0.15815939 0.012848561 -0.22887675 -0.21618247 0.13367777 0.1028471 0.25068823 -0.13625076 -0.11771541 0.4857257 0.102198474 0.06380113 -0.22328818 -0.05281015 0.0059655504 0.095453635 0.39693353 -0.066147 -0.1920163 0.5153346 0.24972811 -0.0076305643 -0.05530072 -0.24668717 -0.074051596 0.29288396 -0.0849124 0.37786478 0.2398532 -0.10374063 0.5445305 -0.41955113 0.39866814 -0.23992492 -0.15373677 0.34488577 -0.07166888 -0.48001364 0.0660652 0.061260436 0.32197484 -0.12741785 0.024006622 -0.07915035 -0.04467735 -0.2387938 -0.07527494 0.07079664 0.074456714 0.17877163 -0.002122373 -0.16164272 0.12381973 -0.5908519 0.5827627 -0.38076186 0.095964395 0.020342976 -0.5244792 0.24467848 -0.12481717 0.2869162 -0.34473857 -0.19579992 -0.18069582 0.015281798 -0.18330036 -0.08794056 0.015334953 -0.5609912 0.17393902 0.04283724 -0.07696586 0.2040299 0.34686008 0.31219167 0.14669564 -0.26249585 -0.42771882 0.5381632 -0.123247474 -0.29142144 -0.29963812 -0.32800657 -0.10684048 -0.08594837 0.19670585 0.13474767 0.18349588 -0.4734125 0.15554792 -0.21062694 -0.14191462 -0.12800062 0.2053445 -0.05258381 0.10878109 0.56381494 0.22724482 -0.17778987 -0.061046753 0.10789692 -0.015310492 0.16563527 -0.31812978 -0.1478078 0.4323269 -0.2543924 -0.25956103 0.38653126 0.5080214 -0.18796602 -0.10318089 0.023921987 -0.14618908 0.22923793 0.37690258 0.13323267 -0.34325415 -0.048353776 -0.30283198 -0.2839813 -0.2627738 -0.07422618 -0.31940162 0.38072023 0.56700015 -0.023362642 -0.3786432 0.084006436 0.0729958 0.09483505 -0.2665334 0.12699558 -0.37927982 -0.39073908 0.0063185897 -0.34464878 -0.24011964 0.09303968 -0.15488827 -0.018486138 0.3560308 -0.26005003 0.089302294 0.116130605 0.07684872 -0.085253105 -0.28178927 -0.17346472 -0.20008522 0.004347025 0.34192443 0.017453942 0.06926512 -0.15926014 -0.018554512 0.18478563 -0.040194467 0.38450953 0.4104423 -0.016453728 0.013374495 -0.011256633 0.09106963 0.20074937 0.17310189 -0.12467103 0.16330549 -0.0009963055 0.12181527 -0.05295286 -0.0059491103 -0.04697837 0.38616535 -0.21074814 -0.32234505 0.47269863 0.27924335 0.13548143 -0.2677968 0.03536313 0.3248672 0.2062973 0.29093853 0.1844036 -0.43359983 0.025519002 -0.06319317 -0.2427806 -0.22732906 0.08803728 -0.041860744 -0.151291 0.3400458 -0.29143015 0.25334117 0.06265491 0.26399022 -0.20121849 0.22156847 -0.50599706 0.069224015 0.52325517 -0.34115726 -0.105219565 -0.37346402 -0.02126528 0.09619415 0.017722093 -0.3621799 -0.109912336 0.021542747 -0.13361925 0.2087667 -0.08780184 0.09494446 -0.25047818 -0.07924239 0.21750642 0.2621652 -0.52888566 0.081884995 -0.20485449 0.18029206 -0.5623824 -0.03897387 0.3213515 0.057455678 -0.26524526 0.14741589 0.1257589 0.04708992 0.026751317 -0.014696863 -0.11038961 0.004459205 -0.01394376 0.091146186 -0.15486309 0.20662159 -0.0987916 -0.07740813 0.009704136 0.28866896 0.3916269 0.35061485 0.31678385 0.43233085 0.44510433\n",
    "\n",
    "For these vectors, I used the industry-standard of 300 dimensions. We see each of these dimensions represented by each of the floats, separated by whitespace. As the model passes over the corpus it is being trained on, it hones these numbers and changes them for each word. Over multiple epochs, or generations, it gains a clearer sense of the similarity of words, or at least words that are used in similar contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "balanced-yield",
   "metadata": {},
   "source": [
    "Once a word vector model is trained, we can do similarity matches very quickly and very reliably. AI work primarily with Holocaust and human rights abuses documents. For this reason, I will use a word vector model that I have trained on Holocaust documents. Consider the word \"concentration camp\". Let‚Äôs now use these word vectors to find the 10 most similar words to concentration camp."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informative-mandate",
   "metadata": {},
   "source": [
    "Once a word vector model is trained, we can do similarity matches very quickly and very reliably. At the start of the notebook, I asked you to consider the word concentration camp. Let‚Äôs now use these word vectors to find the 10 most similar words to concentration camp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "associate-version",
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    ('extermination_camp', 0.5768706798553467),\n",
    "    ('camp', 0.5369070172309875),\n",
    "    ('Flossenbiirg', 0.5099129676818848),\n",
    "    ('Sachsenhausen', 0.5068483948707581),\n",
    "    ('Auschwitz', 0.48929861187934875),\n",
    "    ('Dachau', 0.4765608310699463),\n",
    "    ('concen', 0.4753464460372925),\n",
    "    ('Majdanek', 0.4740387797355652),\n",
    "    ('Sered', 0.47086501121520996),\n",
    "    ('Buchenwald', 0.4692303538322449)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legendary-communications",
   "metadata": {},
   "source": [
    "These are the items that are most similar to concentration camp in our word vectors. The tuple has two indices. Index 0 is the word and index 1 is the similarity, represented as a float.\n",
    "\n",
    "Exterimination camp is not a direct synonym, as it has a distinction in what happened to prisoners, i.e. execution, however, these are very similar. Seeing this as the most similar word is a sign that the word vectors are well-aligned. Camp is expected as it is a singular word that has similar meaning in context to contentration camp. The remainder of this list are proper nouns, all of which were concentration camps with one exception: ‚Äúconcen‚Äù. This is clearly a result of poor cleaning. Concen is not a word, rather a type of concen-tration, most likely. The fact that this is here is also a good sign that our word vectors have aligned well enough to have typos in near vector space.\n",
    "\n",
    "Let‚Äôs do something similar with Auschwitz.items that are most similar to concentration camp in our word vectors. The tuple has two indices. Index 0 is the word and index 1 is the similarity, represented as a float.\n",
    "\n",
    "Exterimination camp is not a direct synonym, as it has a distinction in what happened to prisoners, i.e. execution, however, these are very similar. Seeing this as the most similar word is a sign that the word vectors are well-aligned. Camp is expected as it is a singular word that has similar meaning in context to contentration camp. The remainder of this list are proper nouns, all of which were concentration camps with one exception: ‚Äúconcen‚Äù. This is clearly a result of poor cleaning. Concen is not a word, rather a type of concen-tration, most likely. The fact that this is here is also a good sign that our word vectors have aligned well enough to have typos in near vector space.\n",
    "\n",
    "Let‚Äôs do something similar with Auschwitz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recent-merchant",
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    ('Auschwitz_Birkenau', 0.6649479866027832),\n",
    "    ('Birkenau', 0.5385118126869202),\n",
    "    ('subcamp', 0.5343026518821716),\n",
    "    ('camp', 0.533636748790741),\n",
    "    ('III', 0.5323576927185059),\n",
    "    ('stutthof', 0.518073320388794),\n",
    "    ('Ravensbriick', 0.5084848403930664),\n",
    "    ('Berlitzer', 0.5083401203155518),\n",
    "    ('Malchow', 0.5051567554473877),\n",
    "    ('Oswiecim', 0.5016494393348694)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scheduled-consortium",
   "metadata": {},
   "source": [
    "As we can see, the words closest to Auchwitz are places assocaited with Auschwitz, such as Birkenau, subcamps (of which Auschwitz had many), other concentration camps (such as Ravensbriick), and the location of the Auschwitz memorial, Oswiecim.\n",
    "\n",
    "In other words, we have words closely associated with Auschwitz in particular."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "planned-valley",
   "metadata": {},
   "source": [
    "## Part Nine - Python Libraries for Machine Learning with Texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invisible-wichita",
   "metadata": {},
   "source": [
    "### Beginner Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-statistics",
   "metadata": {},
   "source": [
    "The libraries below are good introductory libraries for learning about machine learning. We will be using two of them in the next two notebooks: Gensim and spaCy. The third library, Stanza, is comporable to spaCy with the exception that it was designed with academia in mind. spaCy, on the other hand, was designed for large datasets and is, therefore, more scalable. It benchmarks better than Stanza and at most tasks achieves better results. Off-the-shelf models from spaCy are also quite good, especially with the new (Feb. 2021) release of spaCy 3 which brings in more robust text machine learning practices, such as BERT models.\n",
    "\n",
    "Gensim is also essential. While you may not be interested in creating a topic model, you will need to create word vectors, something that spaCy cannot yet do. The other library mentioned here is FastText, put out by Facebook (the creators of PyTorch). FastText offers a new and improved way to generate word embeddings, known as sub-word embeddings. These outperform Gensim's Word2Vec model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "paperback-blowing",
   "metadata": {},
   "source": [
    "1) spaCy<br>\n",
    "2) Stanza<br>\n",
    "3) Gensim<br>\n",
    "4) FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitted-puppy",
   "metadata": {},
   "source": [
    "### Intermediate Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "younger-dublin",
   "metadata": {},
   "source": [
    "Once you have the basics of machine learning down, I recommend getting familiar with machine learning algorithms, such as liner regression. At this stage in your learning, you will need to become more familiar with statistics and mathematics. A good way to do this is to follow a series of tutorials on SciKit Learn, the first robust Python library for handling machine learning algorithms.\n",
    "\n",
    "Once you are familiar with the basics, you can begin to do custom tasks with Keras and FastAI. These are wrappers for more robust and complex libraries, TensorFlow and PyTorch, respectively. They allow you to create neural networks in minutes and quickly train custom models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporate-corps",
   "metadata": {},
   "source": [
    "1) SciKit Learn<br>\n",
    "2) Keras<br>\n",
    "3) FastAi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sonic-christianity",
   "metadata": {},
   "source": [
    "### Advanced Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valid-legend",
   "metadata": {},
   "source": [
    "1) TensorFlow<br>\n",
    "2) PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broadband-rover",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vietnamese-flashing",
   "metadata": {},
   "source": [
    "In this notebook, we have learned about what machine learning is and generally how it works. We have also learned a bit about its earlier development from the 1950s to the present. Finally, we have learned the basics of representing words numerically, a fundemantal concept in applying machine learning to texts. In the next two notebooks we will reinforce these skills by solving two text-based problems: topic modeling and text classification."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
