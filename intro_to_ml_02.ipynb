{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning<br><br>Day 02:<br>Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Dr. William Mattingly<br>\n",
    "TAP Institute with JSTOR</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last notebook, we learned about some of the fundamental concepts and terms for engaging in machine learning. In this notebook, we begin to apply those concepts to an unsuperivsed learning problem: topic modeling. This is an unsupervsed learning problem because we will not know the number of topics found within our corpus. Instead, we want to create a model that will cluster the data and find topics (based on a number we assign) across an entire corpus. This notebook is not designed as a course on topic modeling, rather as an introduction to the process of unsupervised learning. I am a firm believer that the best way to understand a new topic is to jump in and engage in that topic. If you are interested in topic modeling more deeply after hearing about it here, I recommend exploring the other TAP Institute course on Topic Modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covered in this Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) What is topic modeling?<br>\n",
    "2) When should I use topic modeling?<br>\n",
    "3) Clusters<br>\n",
    "4) Topics<br>\n",
    "5) The Gensim Library in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background on Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you have forgotten from the last notebook, there are several different ways that we can train machine learning models. The first (explored in this notebook) is unsupervised learning. Another is supervised learning (the following notebook). Another common method is reinforcement learning (not explored here).\n",
    "\n",
    "In this notebook, we will be engaging in unsupervised learning. In this scenario, we do not have labeled data. It is our goal to use a machine learning system to identify patterns in the data and generate the labels for us. Topic Modeling, as we shall see, is an unsupervised learning problem.\n",
    "\n",
    "Unsupervised learning is more commonly used outside of text analysis. It is used by a wide array of disciplines. Let's briefly consider this fun toy example in text (rather than graph and data) form.\n",
    "\n",
    "Imagine that we need to understand what makes certain animals related. We don't know what these animals are, but we want to know if there are patterns that can allow us to identify the types of animals we are looking at. We only know three things about these animals: (1) number of feet; (2) presence of wings; (3) color. It is our gooal to use these features to identify two types of animals.\n",
    "\n",
    "Let's say we have four animals:\n",
    "\n",
    "Animal 1 has two feet, wings, and is white.<br>\n",
    "Animal 2 has two feet, wings, and is brown.<br>\n",
    "Animal 3 has four feet, no wings, and is white<br>\n",
    "Animal 4 has four feet, no wings, and is grey.<br>\n",
    "\n",
    "If we were to cluster these four animals, we as humans could easily place Animal 1 and Animal 2 in a similar category and Animal 3 and Animal 4 in a different one. We can see from these features that color has very limited relevance to cluster these animals because, as we can see, Animal 1 and Animal 3 have the same color, but are different in other features. We as humans from simply reading these descriptions can classify these animals into two distinct categories. Type 1 and Type 2. Type one has four legs and no wings, while Type 2 has two legs and wings. Both types may appear in various colors. At its core, this is an unsupervised learning problem. We do not know the types of animals, but we know the data related to them. We identifiy patterns and then cluster those data about the animals into two distinct categories.\n",
    "\n",
    "What if we introduce a new animal into the mix, the elusive Animal 5, just recently discovered.\n",
    "\n",
    "Animal 5 has four feet, wings, and is white. (Spoiler Alert: It's Pegasus)\n",
    "\n",
    "Were we to try and classify this animal, we now have a problem, right? It doesn't fit the patterns of the others. It may have the same color, but as we've learned color is not a greay feature for classifying our animals. It has a feature from category 1 (four legs) and a feature from category 2 (wings). So, what do we do with it? In an unsupervised learning problem, we may need to adjust our parameters and allow for three types of animals.\n",
    "\n",
    "Text, I hinted in the last notebook and as we will see here again, is far more complex than other machine learning problems because language is inherently more complex. When we think of features in language, we need to think of words, how they are used, and in what contexts. Each word constitutes a specific feature of language and its use needs to be represented numerically. We can achieve the latter with the word embedding we met in the last notebook, but capturing the former is mathematically complex.\n",
    "\n",
    "In this notebook, we will delve a bit deeper into text and machine learning by presenting and solving an unsupervised learning problem known as topic modeling. As we will see, it works similarly to the problem we saw above, though it is magnitudes more complex. We have the data, but we don't know the types (clusters or topics) that exist within that data. It is the unsupervised learning algorithm's job to find those hidden (latent) themes using nothing more the text of an entire corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part One - What is Topic Modeling?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic modeling and text classification (addressed below) are collectively known as \"text categorization\" which is a branch of natural language processing, better known as NLP. It is closely connected to natural language understanding, better known as NLU. NLP is the process by which a researcher uses a computer system to parse human language and extract important metadata from texts. The purpose of NLP is to perform, among other things, distant reding.\n",
    "\n",
    "Distant reading has a long history extending to the late-twentieth century. It is commonly used when the quantity of texts in a given corpus prevent a researcher (or a team of researchers) from reading the corpus closely in its entirety. In order to make sense of that large corpus, the researcher will often pass certain tasks to a computer with the understanding that there is a margin of error. This margin of error is accepted in exchange for the ability to gain a larger, distant understanding of that corpus.\n",
    "\n",
    "The metadata from these tasks can then be used to get a sense of the texts without reading them closely, hence the term distant reading.\n",
    "\n",
    "To get a better understanding of how these fields relate to one another, please see the image below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn-images-1.medium.com/max/1000/1*Uf_qQ0zF8G8y9zUhndA08w.png\" alt=\"fishy\" class=\"bg-primary\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This image is commonly shared across various NLP tutorials and for good reason. It accuarately portrays the diverse field of NLP and its close partner fields of NLU and ASR. The goal of NLU is to give a computer system a text (or collection of texts) and produce some sense of understanding about that text or those texts.\n",
    "\n",
    "There are various types of tasks that fall under NLU, including paraphrase and natural language inference. This is when a computer system takes an input text of, say 5,000 words, reduces that text to its core components, and outputs a summary of the text. This is a task often used by law firms that need to gain a quick understanding of a large corpus of documents to target their investigation and use their time wisely. Another task is sentiment analysis in which a user gives a computer system a text and the system determines whether it is x or y. This is often used by social media companies to determine if a text is abusive so that they can flag and delete inappropriate content automatically.\n",
    "\n",
    "A common form of NLP and the subject of these notebooks is topic modeling and text classification. While closely linked and rather similar, they are distinct methods that perform distinct tasks. For topic modeling, we give a computer system a text and it tells us what topic(s) is (are) discussed in it. For text classification, we give a system a text and it classifies it into certain categories. In essense, while NLP is essential for working with textual data in a computer environment by parsing it and identifying its key components, NLU goes one step further and tries to understand that same data the way a human may.\n",
    "\n",
    "For all NLP and NLU tasks, there are rules-based and machine learning-based approaches. In this notebook, we will be looking at each. Parts Two and Three in this book are focused on clustering and topic modeling. In Part Two, we will explore rules-based methods, such as Term Frequency-Inverse Document Frequency, better known as TF-IDF; and in Part Three we will explore machine learning-based methods, specifically Latent Dirichlet Allocation models, better known as LDA models.\n",
    "\n",
    "Before we move into those subjects, something should be said of rules-based vs. machine learning-based approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A rules-based approach to topic modeling uses a set of rules to extract topics from a text. It does this by identifying keywords in each text in a corpus. One of the most common ways to perform this task is via TF-IDF, or term frequency-inverse document frequency We will not address TF-IDF in these notebooks in great detail, but you should be aware that other non-ML approaches are available and should be used in certain situations. Simply put, a TF-IDF looks for a word’s frequency in a single text, respective to that word’s use across the corpus as a whole. If that word occurs infrequently in all other documents, but frequently in one document, then we use rules to identify the document that sees one word used with a high frequency as the chief document of a given topic.\n",
    "\n",
    "For certain problems, a rules-based approach is particularly useful and appropriate with documents that are shorter, such as tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option to identify topics in a text is via a machine learning-based approach. In this method, we do not give a computer system a set of rules, rather we let the computer generate its own rules to identify topics in a corpus. This is done in two different ways: supervised and unsupervised learning.\n",
    "\n",
    "In supervised learning, we know the key subjects in a corpus. We give a computer system a set of documents with their corresponding label to teach it to identify the characteristics that make that particular topic or class unique. This is mostly used for text classification.\n",
    "\n",
    "Another approach is via unsupervised learning. In unsupervised learning, we do not know the topics of our documents and, instead, we want let the system identify those topics and cluster the ones of a highd degree of similarity together. We then examine the words that occur the most frequently in each cluster to get a sense of the topics at hand. The classic example for machine learning topic modeling is LDA, or Latent Dirichlet Allocation. We will learn about this method in far more detail in Part Three."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Two - When to Use Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of this leads to a vital question: Why use topic modeling? Topic modeling affords researchers the ability to learn a lot about their corpus very quickly. It is often used whent the corpus is so large that no single human could read it in a single lifetime.\n",
    "\n",
    "In both a rules-based and machine learning-based approach, a researcher can see what major subjects are discussed in a corpus. This information can be used to perform targetted research by weeding out the documents that likely do not contain the information the researcher needs. Additionally, the information drawn from topic modeling can be used to make large deductions about the corpus at hand. We will see that topic modeling can be used to draw imprecise or incorrect conclusions.\n",
    "\n",
    "It is vital, however, to understand the limitations of topic modeling. There is always a potential for the researcher to use topic modeling to validate a wrong presumption about the data. Throughout this series, I will emphasize methodological steps that can (and should) be taken to limit these mistakes. Despite this potential for error, topic modeling can provide valuable insight, relatively quickly about a large corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Three - What are Clusters and Topics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of topic modeling is to assign topics to a large corpus with each document recieving one topic. <u><b>Topics</b> are labels assigned to textual data that detail the subjects contained within a given text.</u> In topic modeling we try to create computer systems that can assign topics the way a human would. In order to understand this process, it’s best if we take a step back and think about how we assign topics.\n",
    "\n",
    "To do this, let’s examine these two texts.\n",
    "\n",
    "Text 1: Thomas enjoys playing basketball. He is an exceptionally good point guard.<br>\n",
    "Text 2: Victoria enjoys playing baseball. She is an exceptionally good at playing first base.\n",
    "\n",
    "If I asked you to provide two topics to these texts, what might they be? Basketball and baseball are likely two top candidates. Text 1 would have the topic of basketball, while text 2 would have the topic of baseball. Now, let’s consider these same texts, but add two more into the mix.\n",
    "\n",
    "Text 3: John is a talented chef. He enjoys making pasta professionally.<br>\n",
    "Text 4: Jeff is a talented cook. He owns a pizzaria.\n",
    "\n",
    "Now, if I asked you to assign two topics to all four texts, what might those topics be? It is likely that your answer changed. No longer are the two topics of baseball and basketball relevant because Text 3 and Text 4 do not align well with those topics. Instead, a better pair of topics might be sports and cooking, or something like that. What changed? The collection of texts in our corpus changed.\n",
    "\n",
    "What does this demonstrate? It tells us that topics are corpus-dependent, meaning the topics we assign to texts depend on their context against surrounding texts. The same holds true for topic modeling via computer systems.\n",
    "\n",
    "In topic modeling, computer systems do not generate topics, rather they generate a list of high concentration words. Texts that share common terms are clustered together by similarity. A cluster is nothing more than a collection of similar texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Four - Introduction to Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim is a powerful Python library that was originally designed to produce good topic models. Topic models are machine learning models that read over an entire corpus and cluster individual documents into clusters of similarity. In order to produce good results, Gensim (and other topic modeling methods) are reliant upon numerical represntations of words. In other words, these methods depend on word vectors. To have accurate results, therefore, Gensim is capable of generating word vectors with relatively minimal code. SpaCy, on the other hand, is an NLP library not capable of generating custom word vectors. While users can inject to words into models, spaCy is not designed to generate word vectors on its own. For this reason, even spaCy’s documentation recomends using other libraries, such as Gensim to generate word vectors.\n",
    "\n",
    "In this notebook, we will be going through the process of generating our own word vectors. In order to reduce the time to perform the task at hand, we will use a toy corpus. This process, however, can easily be scaled for a corpus of millions of documents.\n",
    "\n",
    "In order to generate word vectors, we need one thing: a corpus/ Let’s create one right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"Tom is cat, while Jerry is a mouse. Tom and Jerry are characters in a cartoon series. Some of the cartoons contain words, but most are silent. Silent cartoons still have music and sound effects.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can give this corpus to Gensim, however, we need to do a few preprocessing techniques to it.\n",
    "\n",
    "1. First, we need to remove the stopwords from the corpus. Stopwords are words that occur frequently in a corpus, so frequently that they do not necessarily offer much meaning for distant reading and, as a result, throw off machine learning models. Other stopwords are words that occur with high frequency in a langauge as a whole. For our purposes, we will use the following stopwords available from the NLTK (natural language toolkit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [\"i\",\"me\",\"my\",\"myself\",\"we\",\"our\",\"ours\",\"ourselves\",\"you\",\"your\",\"yours\",\"yourself\",\"yourselves\",\n",
    "             \"he\",\"him\",\"his\",\"himself\",\"she\",\"her\",\"hers\",\"herself\",\"it\",\"its\",\"itself\",\"they\",\"them\",\"their\",\n",
    "             \"theirs\",\"themselves\",\"what\",\"which\",\"who\",\"whom\",\"this\",\"that\",\"these\",\"those\",\"am\",\"is\",\"are\",\"was\",\n",
    "             \"were\",\"be\",\"been\",\"being\",\"have\",\"has\",\"had\",\"having\",\"do\",\"does\",\"did\",\"doing\",\"a\",\"an\",\"the\",\"and\",\n",
    "             \"but\",\"if\",\"or\",\"because\",\"as\",\"until\",\"while\",\"of\",\"at\",\"by\",\"for\",\"with\",\"about\",\"against\",\"between\",\n",
    "             \"into\",\"through\",\"during\",\"before\",\"after\",\"above\",\"below\",\"to\",\"from\",\"up\",\"down\",\"in\",\"out\",\"on\",\"off\",\n",
    "             \"over\",\"under\",\"again\",\"further\",\"then\",\"once\",\"here\",\"there\",\"when\",\"where\",\"why\",\"how\",\"all\",\"any\",\"both\",\n",
    "             \"each\",\"few\",\"more\",\"most\",\"other\",\"some\",\"such\",\"no\",\"nor\",\"not\",\"only\",\"own\",\"same\",\"so\",\"than\",\"too\",\"very\",\n",
    "             \"s\",\"t\",\"can\",\"will\",\"just\",\"don\",\"should\",\"now\"\n",
    "            ]\n",
    "corpus = corpus.lower()\n",
    "words = corpus.split()\n",
    "\n",
    "new_corpus = []\n",
    "for word in words:\n",
    "    if word not in stopwords:\n",
    "        new_corpus.append(word)\n",
    "\n",
    "corpus = \" \".join(new_corpus)\n",
    "print (corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Second, this corpus should be divided into sentences. In order to do that, I recommend using spaCy’s sentence tokenizer.<br>\n",
    "3. While we do this, we should also eliminate the punctuation from the sentences. We can do this with the standard string library from Python.<br>\n",
    "4. Also at this stage, we should lowercase our words (OPTIONAL)<br>\n",
    "4. If we wish to produce a smaller amount of word vectors, we could also consider lemmatizing our words as well (OPTIONAL)<br>\n",
    "5. We need to split the sentence into words and append that list of words to a new object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install spaCy\n",
    "!pip install -U pip setuptools wheel\n",
    "!pip install -U spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import string\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(corpus)\n",
    "\n",
    "sentences = []\n",
    "for sent in doc.sents:\n",
    "    sentence = sent.text.translate(str.maketrans('', '', string.punctuation))\n",
    "    words = sentence.split()\n",
    "    sentences.append(words)\n",
    "print (sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Five - Creating Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, we can start preparing our word vectors. To do this, we will use the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wordvecs(corpus, model_name):\n",
    "    from gensim.models.word2vec import Word2Vec\n",
    "    w2v_model = Word2Vec(min_count=1)\n",
    "    w2v_model.build_vocab(sentences)\n",
    "    w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "    w2v_model.wv.save_word2vec_format(f\"data/{model_name}.txt\")\n",
    "create_wordvecs(sentences, \"word_vecs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can open up our word vectors and examine them. The first line in this text file will be the shape of the word vectors. This should be two integers. The first number (17) is the number of unique words in the vocabulary. The second number (10) are the number of dimensions of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"data/word_vecs.txt\", \"r\") as f:\n",
    "    data = f.readlines()\n",
    "    print (data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s look at the first word in our word vectors, “silent”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " print (data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we see two pieces of information. The first is a string and it is the word itself. In this case, “silent”. The second bit of data is a series of 10 floats. These are our dimensions for the word. This is the numerical way in which “silent” is understood by the Gensim model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Six - Topic Modeling with Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these understandings now down, it is time to engage in topic modeling with Gensim. To do this, we will use a sample dataset known as the 20-newsgroups dataset. In what follows, we will largely follow the standard Gensim tutorial: https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/ with significant modifications. My modifications are designed to reduce the complexity of topic modeling for entry-level students. I have also updated the visualization code.\n",
    "\n",
    "For our purposes, we will be using a topic model known as an LDA Topic Model. LDA stands for Latent Dirichlet Allocation. This name may be a bit scary at first, but a good way to think of it as a way to identify latent, or hidden, topics in a text.\n",
    "\n",
    "We first need to load in the data. To do that, we will use Pandas and its built in read_json() function. This will allow us to grab the data from the internet and automatically format it for us to work with it in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json('https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the we have the data loaded, let's convert it to a list.\n",
    "\n",
    "For those unfamiliar or new to data analysis, you will find that the majority of your time is spent cleaning datasets or preparing uncultivated data to fulfill specific purposes. This dataset, despite being well-cultivated, is no exception. We need to clean it and isolate the essential message that we wish to perform topic modeling on.\n",
    "\n",
    "To do this, let's examine a sample item from this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data = df.content.values.tolist()\n",
    "print (news_data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each item in the dataset will have some preliminary header data (which we do not need), separated by a double line break. That which comes after the double line break is the key piece of the message that we are interested in. We are also not interested in email addresses. With this bit of information, we can write a few rules to remove the header and clean up the data. In addition to that, we can leverage the power of Gensim's built in simple_preprocess() function that allows us to easily clean and structure texts for topic modeling.\n",
    "\n",
    "For those familiar with Python, the code below may look exceptionally long. The code that follows is not what I would call \"efficient\" nor is it the most Pythonic way to achieve the tasks at hand. It is, however, easier to parse for new coders. I have, therefore, opted to not use list comprehension and be more verbose in my code so that all readers will be able to more easily follow along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from gensim.utils import simple_preprocess\n",
    "final = []\n",
    "for item in news_data:\n",
    "    #separate the header from the main body of the text\n",
    "    item = item.split(\"\\n\\n\", 1)[1]\n",
    "    \n",
    "    #remove all email addresses with Regex\n",
    "    item = re.sub('\\S*@\\S*\\s?', '', item)\n",
    "    \n",
    "    #create a list of words by removing all non-text material, specifically punctuation and numbers\n",
    "    words = simple_preprocess(item)\n",
    "    \n",
    "    #create an empty list to append to\n",
    "    final_words = []\n",
    "    \n",
    "    #make sure all the words are not stopwords (the list we created above)\n",
    "    for word in words:\n",
    "        if word not in stopwords:\n",
    "            \n",
    "            #eliminate short words. This dataset has a lot of \"rx\", \"rz\", etc. data that may throw off the model.\n",
    "            if len(word) > 3:\n",
    "                words = final_words.append(word)\n",
    "    final.append(final_words)\n",
    "print (final[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset now is properly cleaned and in the format Gensim expects. Now, it's time to create a model. We will not be finetuning this model or exploring all the various hyperparameters available to researchers. Instead, we will create a basic model. I always like to do this as a first pass with my topic modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "id2word = corpora.Dictionary(final)\n",
    "corpus = [id2word.doc2bow(text) for text in final]\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=20, chunksize=100, passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = lda_model.print_topics()\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This data is rather difficult to process. I always like to use the handy and useful Python library PyLDAvis to visualize my topic modeling data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pyLDAvis allows us to easily interpret the model's results. We can see that through unsupervised learning the model has found for us the desired 20 clusters (or topics) of data. It now falls on us, as the rersearcher, to interpret these clusters. If I were to assign a label to Cluster 7, for example, I might assign the word \"religious\". This is because the words that are associated with cluster 7 are clearly religious in nature. Cluster 9, likewise, deals with sports. Cluster 8 deals with encryption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Eight - Improving an LDA Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the notable problems with this result is the different conjugation forms of stop words, specifically the verb \"to be\". This can be seen in the high priority of Cluster 4 with the word \"would\". One of the ways to improve results is through something known as lemmatization, or the reduction of a word to a specific root, or lemma. This takes all the forms of a specific verb or noun and reduces them to a single standard form. The verbs \"would\", \"was, and \"am\", therefore, all become \"is\" in lemmatization. Likewise, the words \"teams\" and \"team\" become \"team\". Lemmatization reduces a text further and typically improves the results.\n",
    "\n",
    "Lemmatization can, however, be computationally expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams and Trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another noticable problem with the above approach iis that every word used to perform clustering is singular. We know that words used collectively have different meanings than their individual counterparts.\n",
    "\n",
    "Let’s take a moment and step away from the subject of this textbook, topic modeling. Instead, let’s think about language, the essential medium of topic modeling. This notebook will be exclusively about one aspect of langauge: bigrams and trigrams. When we use words, those words correspond to something distinct.\n",
    "\n",
    "If I use the word apple, you likely are thinking of something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"https://images-na.ssl-images-amazon.com/images/I/81Dl5GdAVkL.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apple is a simple word, yet it can mean different things in different contexts. What if I said the following: “My Apple is better than a PC.” Now, what image comes to mind? Perhaps this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"https://images-na.ssl-images-amazon.com/images/I/71pheYd9W0L._AC_SL1500_.jpg\", width=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of textual ambiguity. Syntactical context helps eliminate textual ambiguity. Apple is a single word here that contains a single concept. It's a relatively simple word. Perhaps one of the earliest a native speaker of English learns as a child. And yet it has textual ambiguity. Because \"apple\" is a single word, it is known as a unigram. A **unigram** is a single word that represents a single concept.\n",
    "Textual ambiguity, however, occurs in more dynamic ways when we think about concepts beyond the single span of a single word. In this notebook, we will focus on two such cases that are essential for natural language processing: bigrams and trigrams. Bigrams are two words that contain a distinct meaning when used together, while trigrams are three words that contain a distinct meaning when used together.\n",
    "\n",
    "Understanding bigrams and trigrams are essential because in order for a computer to truly understand langauge the way a human does, it must be able to understand the nuances of a single word and how a word’s meaning not only shifts in context, but shifts in meaning when used in conjunction with other words.\n",
    "\n",
    "As noted above, a bigram is a combination of two words that have a distinct meaning. To demonstrate this, let us consider quickly the word “French”. A single word, that may have multiple meanings. Perhaps the word French refers to the language:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"https://www.unb.ca/cel/_assets/images/enrichment/ll-fred/ll-french-beginners.jpg\", width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps it references a French person:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"https://images.movehub.com/wp-content/uploads/2017/09/15150656/living-in-france.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's hold off on the word French for just a moment. Let's now use the word \"revolution\". Again, there is textual ambiguity, but perhaps I am referencing the concept of revolution in the sense of the Earth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"https://i.ytimg.com/vi/CqkQv617bcw/maxresdefault.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you may already see where I am going with this, but let’s now think about what happens when I put those two textually ambiguous unigrams together “The French Revolution”. “The” here is a stop word that is frequently dropped in natural language processing, so “French Revolution” is all that we should consider. This two words when combined have a distinct concept. Now, you may be thinking of this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we can think about language in even more nuanced ways.\n",
    "\n",
    "Trigrams, as noted above, are the same as bigrams, except with three words, instead of two. Let’s continue with our example of “French”. What might you think about if I used the word “army”. Perhaps something distinct to your own experiences with the word. For me, as a modern American, I think initially about the American Army in the modern sense of the word. So I may picture something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"https://www.goarmy.com/content/dam/goarmy/refresh/redesign-nav-images/Desktop_Navigation_IMG_1.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For others in other parts of the world, other images may be more relevant. However, when I use the phrase \"The French Revolutionary Army\", I now have something defined, something distinct. I have this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"https://upload.wikimedia.org/wikipedia/commons/4/47/Bataille_Jemmapes.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a trigram, a distinct concept consisting of three words that may have individual meanings when used alone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, why are bigrams and trigrams so important? The reason comes down to getting machines to understand that when certain words are used together, they bear a distinct meaning. In order to produce a good topic model, therefore, the model must be able to understand and process words in this manner, the way we humans use the language we are trying to get the machine to understand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Nine - Creating an Improved LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
