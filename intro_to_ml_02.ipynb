{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "basic-calibration",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning<br><br>Day 02:<br>Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capital-telephone",
   "metadata": {},
   "source": [
    "<center>Dr. William Mattingly<br>\n",
    "TAP Institute with JSTOR</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formed-calibration",
   "metadata": {},
   "source": [
    "## Preface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consolidated-reminder",
   "metadata": {},
   "source": [
    "In the last notebook, we learned about some of the fundamental concepts and terms for engaging in machine learning. In this notebook, we begin to apply those concepts to an unsuperivsed learning problem: topic modeling. This is an unsupervsed learning problem because we will not know the number of topics found within our corpus. Instead, we want to create a model that will cluster the data and find topics (based on a number we assign) across an entire corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-triple",
   "metadata": {},
   "source": [
    "## Covered in this Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opening-creation",
   "metadata": {},
   "source": [
    "1) What is topic modeling?<br>\n",
    "2) When should I use topic modeling?<br>\n",
    "3) Clusters<br>\n",
    "4) Topics<br>\n",
    "5) The Gensim Library in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompt-botswana",
   "metadata": {},
   "source": [
    "## Part One - What is Topic Modeling?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selective-merit",
   "metadata": {},
   "source": [
    "Topic modeling and text classification (addressed below) is a branch of natural language understanding, better known as NLP. It is closely connected to natural language understanding, better known as NLU. NLP is the process by which a researcher uses a computer system to parse human language and extract important metadata from texts. The purpose of NLP is to perform, among other things, distant reding.\n",
    "\n",
    "Distant reading has a long history extending to the late-twentieth century. It is commonly used when the quantity of texts in a given corpus prevent a researcher (or a team of researchers) from reading the corpus closely in its entirety. In order to make sense of that large corpus, the researcher will often pass certain tasks to a computer with the understanding that there is a margin of error. This margin of error is accepted in exchange for the ability to gain a larger, distant understanding of that corpus.\n",
    "\n",
    "The metadata from these tasks can then be used to get a sense of the texts without reading them closely, hence the term distant reading.\n",
    "\n",
    "To get a better understanding of how these fields relate to one another, please see the image below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closed-annotation",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn-images-1.medium.com/max/1000/1*Uf_qQ0zF8G8y9zUhndA08w.png\" alt=\"fishy\" class=\"bg-primary\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arctic-fireplace",
   "metadata": {},
   "source": [
    "This image is commonly shared across various NLP tutorials and for good reason. It accuarately portrays the diverse field of NLP and its close partner fields of NLU and ASR. The goal of NLU is to give a computer system a text (or collection of texts) and produce some sense of understanding about that text or those texts.\n",
    "\n",
    "There are various types of tasks that fall under NLU, including paraphrase and natural language inference. This is when a computer system takes an input text of, say 5,000 words, reduces that text to its core components, and outputs a summary of the text. This is a task often used by law firms that need to gain a quick understanding of a large corpus of documents to target their investigation and use their time wisely. Another task is sentiment analysis in which a user gives a computer system a text and the system determines whether it is x or y. This is often used by social media companies to determine if a text is abusive so that they can flag and delete inappropriate content automatically.\n",
    "\n",
    "A common form of NLP and the subject of these notebooks is topic modeling and text classification. While closely linked and rather similar, they are distinct methods that perform distinct tasks. For topic modeling, we give a computer system a text and it tells us what topic(s) is (are) discussed in it. For text classification, we give a system a text and it classifies it into certain categories. In essense, while NLP is essential for working with textual data in a computer environment by parsing it and identifying its key components, NLU goes one step further and tries to understand that same data the way a human may.\n",
    "\n",
    "For all NLP and NLU tasks, there are rules-based and machine learning-based approaches. In this notebook, we will be looking at each. Parts Two and Three in this book are focused on clustering and topic modeling. In Part Two, we will explore rules-based methods, such as Term Frequency-Inverse Document Frequency, better known as TF-IDF; and in Part Three we will explore machine learning-based methods, specifically Latent Dirichlet Allocation models, better known as LDA models.\n",
    "\n",
    "Before we move into those subjects, something should be said of rules-based vs. machine learning-based approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bulgarian-update",
   "metadata": {},
   "source": [
    "A rules-based approach to topic modeling uses a set of rules to extract topics from a text. It does this by identifying keywords in each text in a corpus. One of the most common ways to perform this task is via TF-IDF, or term frequency-inverse document frequency. We will discuss this method a lot more in Part Two of these notebooks. Simply put, a TF-IDF looks for a word’s frequency in a single text, respective to that word’s use across the corpus as a whole. If that word occurs infrequently in all other documents, but frequently in one document, then we use rules to identify the document that sees one word used with a high frequency as the chief document of a given topic.\n",
    "\n",
    "For certain problems, a rules-based approach is particularly useful. As we will see, documents that are shorter, such as tweets, tend to fare better from rules-based approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invalid-token",
   "metadata": {},
   "source": [
    "Another option to identify topics in a text is via a machine learning-based approach. In this method, we do not give a computer system a set of rules, rather we let the computer generate its own rules to identify topics in a corpus. This is done in two different ways: supervised and unsupervised learning.\n",
    "\n",
    "In supervised learning, we know the key subjects in a corpus. We give a computer system a set of documents with their corresponding label to teach it to identify the characteristics that make that particular topic or class unique. This is mostly used for text classification.\n",
    "\n",
    "Another approach is via unsupervised learning. In unsupervised learning, we do not know the topics of our documents and, instead, we want let the system identify those topics and cluster the ones of a highd degree of similarity together. We then examine the words that occur the most frequently in each cluster to get a sense of the topics at hand. The classic example for machine learning topic modeling is LDA, or Latent Dirichlet Allocation. We will learn about this method in far more detail in Part Three."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toxic-saturn",
   "metadata": {},
   "source": [
    "## Part Two - When to Use Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graduate-ground",
   "metadata": {},
   "source": [
    "All of this leads to a vital question: Why use topic modeling? Topic modeling affords researchers the ability to learn a lot about their corpus very quickly. It is often used whent he corpus is so large that no single human could read it in a single lifetime.\n",
    "\n",
    "In both a rules-based and machine learning-based approach, a researcher can see what major subjects are discussed in a corpus. This information can be used to perform targetted research by weeding out the documents that likely do not contain the information the researcher needs. Additionally, the information drawn from topic modeling can be used to make large deductions about the corpus at hand. We will see that topic modeling can be used to draw imprecise or incorrect conclusions.\n",
    "\n",
    "It is vital, however, to understand the limitations of topic modeling. There is always a potential for the researcher to use topic modeling to validate a wrong presumption about the data. Throughout this series, I will emphasize methodological steps that can (and should) be taken to limit these mistakes. Despite this potential for error, topic modeling can provide valuable insight, relatively quickly about a large corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polish-infrared",
   "metadata": {},
   "source": [
    "## Part Three - What are Clusters and Topics?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "taken-commodity",
   "metadata": {},
   "source": [
    "Topics are labels assigned to textual data that detail the subjects contained within a given text. In topic modeling we try to create computer systems that can assign topics the way a human would. In order to understand this process, it’s best if we take a step back and think about how we assign topics.\n",
    "\n",
    "To do this, let’s examine these two texts.\n",
    "\n",
    "Text 1: Thomas enjoys playing basketball. He is an exceptionally good point guard.<br>\n",
    "Text 2: Victoria enjoys playing baseball. She is an exceptionally good at playing first base.\n",
    "\n",
    "If I asked you to provide two topics to these texts, what might they be? Basketball and baseball are likely two top candidates. Text 1 would have the topic of basketball, while text 2 would have the topic of baseball. Now, let’s consider these same texts, but add two more into the mix.\n",
    "\n",
    "Text 3: John is a talented chef. He enjoys making pasta professionally.<br>\n",
    "Text 4: Jeff is a talented cook. He owns a pizzaria.\n",
    "\n",
    "Now, if I asked you to assign two topics to all four texts, what might those topics be? It is likely that your answer changed. No longer are the two topics of baseball and basketball relevant because Text 3 and Text 4 do not align well with those topics. Instead, a better pair of topics might be sports and cooking, or something like that. What changed? The collection of texts in our corpus changed.\n",
    "\n",
    "What does this demonstrate? It tells us that topics are corpus-dependent, meaning the topics we assign to texts depend on their context against surrounding texts. The same holds true for topic modeling via computer systems.\n",
    "\n",
    "In topic modeling, computer systems do not generate topics, rather they generate a list of high concentration words. Texts that share common terms are clustered together by similarity. A cluster is nothing more than a collection of similar texts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portuguese-december",
   "metadata": {},
   "source": [
    "## Part Four - Introduction to Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disabled-cherry",
   "metadata": {},
   "source": [
    "Gensim is a powerful Python library that was originally designed to produce good topic models. Topic models are machine learning models that read over an entire corpus and cluster individual documents into clusters of similarity. In order to produce good results, Gensim (and other topic modeling methods) are reliant upon numerical represntations of words. In other words, these methods depend on word vectors. To have accurate results, therefore, Gensim is capable of generating word vectors with relatively minimal code. SpaCy, on the other hand, is an NLP library not capable of generating custom word vectors. While users can inject to words into models, spaCy is not designed to generate word vectors on its own. For this reason, even spaCy’s documentation recomends using other libraries, such as Gensim to generate word vectors.\n",
    "\n",
    "In this notebook, we will be going through the process of generating our own word vectors. In order to reduce the time to perform the task at hand, we will use a toy corpus. This process, however, can easily be scaled for a corpus of millions of documents.\n",
    "\n",
    "In order to generate word vectors, we need one thing: a corpus/ Let’s create one right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "indian-mystery",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"Tom is cat, while Jerry is a mouse. Tom and Jerry are characters in a cartoon series. Some of the cartoons contain words, but most are silent. Silent cartoons still have music and sound effects.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-survival",
   "metadata": {},
   "source": [
    "Before we can give this corpus to Gensim, however, we need to do a few preprocessing techniques to it.\n",
    "\n",
    "1. First, we need to remove the stopwords from the corpus. Stopwords are words that occur frequently in a corpus, so frequently that they do not necessarily offer much meaning for distant reading and, as a result, throw off machine learning models. Other stopwords are words that occur with high frequency in a langauge as a whole. For our purposes, we will use the following stopwords available from the NLTK (natural language toolkit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dominican-expansion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tom cat, jerry mouse. tom jerry characters cartoon series. cartoons contain words, silent. silent cartoons still music sound effects.\n"
     ]
    }
   ],
   "source": [
    "stopwords = [\"i\",\"me\",\"my\",\"myself\",\"we\",\"our\",\"ours\",\"ourselves\",\"you\",\"your\",\"yours\",\"yourself\",\"yourselves\",\n",
    "             \"he\",\"him\",\"his\",\"himself\",\"she\",\"her\",\"hers\",\"herself\",\"it\",\"its\",\"itself\",\"they\",\"them\",\"their\",\n",
    "             \"theirs\",\"themselves\",\"what\",\"which\",\"who\",\"whom\",\"this\",\"that\",\"these\",\"those\",\"am\",\"is\",\"are\",\"was\",\n",
    "             \"were\",\"be\",\"been\",\"being\",\"have\",\"has\",\"had\",\"having\",\"do\",\"does\",\"did\",\"doing\",\"a\",\"an\",\"the\",\"and\",\n",
    "             \"but\",\"if\",\"or\",\"because\",\"as\",\"until\",\"while\",\"of\",\"at\",\"by\",\"for\",\"with\",\"about\",\"against\",\"between\",\n",
    "             \"into\",\"through\",\"during\",\"before\",\"after\",\"above\",\"below\",\"to\",\"from\",\"up\",\"down\",\"in\",\"out\",\"on\",\"off\",\n",
    "             \"over\",\"under\",\"again\",\"further\",\"then\",\"once\",\"here\",\"there\",\"when\",\"where\",\"why\",\"how\",\"all\",\"any\",\"both\",\n",
    "             \"each\",\"few\",\"more\",\"most\",\"other\",\"some\",\"such\",\"no\",\"nor\",\"not\",\"only\",\"own\",\"same\",\"so\",\"than\",\"too\",\"very\",\n",
    "             \"s\",\"t\",\"can\",\"will\",\"just\",\"don\",\"should\",\"now\"\n",
    "            ]\n",
    "corpus = corpus.lower()\n",
    "words = corpus.split()\n",
    "\n",
    "new_corpus = []\n",
    "for word in words:\n",
    "    if word not in stopwords:\n",
    "        new_corpus.append(word)\n",
    "\n",
    "corpus = \" \".join(new_corpus)\n",
    "print (corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serial-departure",
   "metadata": {},
   "source": [
    "2. Second, this corpus should be divided into sentences. In order to do that, I recommend using spaCy’s sentence tokenizer.<br>\n",
    "3. While we do this, we should also eliminate the punctuation from the sentences. We can do this with the standard string library from Python.<br>\n",
    "4. Also at this stage, we should lowercase our words (OPTIONAL)<br>\n",
    "4. If we wish to produce a smaller amount of word vectors, we could also consider lemmatizing our words as well (OPTIONAL)<br>\n",
    "5. We need to split the sentence into words and append that list of words to a new object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "royal-consumption",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['tom', 'cat', 'jerry', 'mouse'], ['tom', 'jerry', 'characters', 'cartoon', 'series'], ['cartoons', 'contain', 'words', 'silent'], ['silent', 'cartoons', 'still', 'music', 'sound', 'effects']]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import string\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(corpus)\n",
    "\n",
    "sentences = []\n",
    "for sent in doc.sents:\n",
    "    sentence = sent.text.translate(str.maketrans('', '', string.punctuation))\n",
    "    words = sentence.split()\n",
    "    sentences.append(words)\n",
    "print (sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artistic-difference",
   "metadata": {},
   "source": [
    "## Part Five - Creating Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developmental-genre",
   "metadata": {},
   "source": [
    "At this stage, we can start preparing our word vectors. To do this, we will use the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "false-kitchen",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wordvecs(corpus, model_name):\n",
    "    from gensim.models.word2vec import Word2Vec\n",
    "    w2v_model = Word2Vec(min_count=1)\n",
    "    w2v_model.build_vocab(sentences)\n",
    "    w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "    w2v_model.wv.save_word2vec_format(f\"data/{model_name}.txt\")\n",
    "create_wordvecs(sentences, \"word_vecs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documentary-madonna",
   "metadata": {},
   "source": [
    "Now, we can open up our word vectors and examine them. The first line in this text file will be the shape of the word vectors. This should be two integers. The first number (17) is the number of unique words in the vocabulary. The second number (10) are the number of dimensions of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "annual-kansas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open (\"data/word_vecs.txt\", \"r\") as f:\n",
    "    data = f.readlines()\n",
    "    print (data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yellow-tower",
   "metadata": {},
   "source": [
    "Let’s look at the first word in our word vectors, “silent”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "neural-verse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "silent -0.00054096937 0.00023816715 0.0051021995 0.009006135 -0.009300297 -0.0071164677 0.0064598355 0.00897975 -0.0050193104 -0.0037623234 0.0073800264 -0.0015330206 -0.0045433184 0.0065578683 -0.004862911 -0.001817435 0.0028776203 0.0009981464 -0.008280853 -0.009456136 0.0073130513 0.0050611165 0.0067697237 0.0007680518 0.0063493964 -0.003403032 -0.00094805425 0.0057731112 -0.0075262673 -0.0039340584 -0.007505744 -0.0009285571 0.009542602 -0.0073249387 -0.002325071 -0.0019412229 0.008077628 -0.0059301443 4.4383218e-05 -0.0047524283 -0.009602784 0.0050010644 -0.008770156 -0.004384546 -3.431853e-05 -0.00030049618 -0.007657594 0.009613245 0.004982054 0.009229672 -0.008153839 0.0044951704 -0.0041342853 0.00082299445 0.008498835 -0.004468288 0.004511762 -0.0067935865 -0.0035477346 0.009396493 -0.0015808161 0.00031599196 -0.0041384036 -0.007684123 -0.0015134802 0.0024720856 -0.00087977306 0.005536616 -0.0027453955 0.0022682755 0.0054518464 0.008352847 -0.001456623 -0.0092013 0.0043750172 0.00057682244 0.0074403184 -0.00080875977 -0.0026344808 -0.008758771 -0.000863235 0.0028255943 0.005407263 0.0070528826 -0.005711897 0.0018537375 0.0060960655 -0.0048010536 -0.0031143127 0.0067949467 0.001636128 0.00018571544 0.003478543 0.00021305577 0.009619703 0.0050555817 -0.00890786 -0.0070408345 0.00089970743 0.006396157\n",
      "\n"
     ]
    }
   ],
   "source": [
    " print (data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "available-malawi",
   "metadata": {},
   "source": [
    "Here, we see two pieces of information. The first is a string and it is the word itself. In this case, “silent”. The second bit of data is a series of 10 floats. These are our dimensions for the word. This is the numerical way in which “silent” is understood by the Gensim model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "future-functionality",
   "metadata": {},
   "source": [
    "# W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unusual-slide",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monthly-clone",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "human-modem",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satellite-spice",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recovered-norfolk",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "further-transfer",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
